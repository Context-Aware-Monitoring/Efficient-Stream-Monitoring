from models.policy import RandomPolicy, MPTS, PushMPTS, AbstractContextualBandit, EGreedy, DKEGreedy, CDKEGreedy, CPushMpts
import numpy as np
from matplotlib import pyplot as plt
import pandas as pd
import yaml
from random import seed, random, randint
import time
from datetime import datetime
import sys
sys.path.append("./pre_processing")


LOWER_BOUND_SAMPLE_SIZE_KMEANS = 10


def _read_epsilon_from_config(config):
    if bool(config.get('epsilon')):
        return config['epsilon']

    return 0.0


def get_cross_validated_policies(config_for_policy, params):
    """Creates policies with different combinations of parameters.
    The config_for_policy is completed with the different combinations of
    parameters in the params dictionary.

    Args:
      config_for_policy (dict): Config of a policy. E.g: {'name' : 'egreedy'}
      params (dict): Keys in the dict correspond to parameter names that will
      be filled into the config. Values correspond to arrays that contain
      different values. E.g.: {'epsilon': [0,0.5]}

    Returns:
      dict[]: All possible configurations of policies that get generated by using
      different combinations of parameters in the params dict.
      Here: [{'name': 'egreedy', 'epsilon': 0},
             {'name': 'egreedy', 'epsilon': 0.5}]
    """
    policies = []
    if len(params.keys()) == 1:
        for k, values in params.items():
            for current_value in values:
                policies.append(config_for_policy | {k: current_value})
    else:
        k1 = list(params.keys())[0]
        v1 = params[k1]
        updated_params = dict(params)
        del updated_params[k1]
        for current_value in v1:
            new_config = config_for_policy | {k1: current_value}
            policies.extend(get_cross_validated_policies(
                new_config, updated_params))

    return policies


class Experiment:
    """Performs an experiment where multiple different policies are run and
    can be compared against each other. The configuration for the experiment is
    read from a yaml file.

    Attributes:
    ___________
    _reward_df (DataFrame): DataFrame containing the reward. This is read from
    a reward csv file provided in the config.yml.
    _K (int): Number of total arms
    _L (int): Number of arms to pick each iteration
    _T (int): Total number of iterations
    _policies (AbstractBandit[]): Different policies that are executed
    _context (float[][]): Contains the context for each of the policies. Context
    is a vector of floats.
    _seed (int): Seed for random generator
    Methods:
    ________
    run():
        Runs the experiment
    get_policies():
        Returns _policies
    get_top_correlated_arms(t, names):
        Returns either the names or the indicies of the top L correlated arms
        for iteration t.
    """

    def __init__(self, config_path=None, additional_config={}):
        """Constructs the Experiment instance from the passed yaml file.
        Further configurations can be direcly passed too.

        Args:
          config_path (string): Path to a yaml config file
          additional_config (dict): Additional configuration
        """
        config = additional_config
        if config_path is not None:
            with open(config_path) as yml_file:
                config = config | yaml.safe_load(yml_file)

        if config.get('reward_path') is None:
            sys.exit('Config file does not containg reward csv file')

        filepath_rewards = config['reward_path']
        self._reward_df = pd.read_csv(filepath_rewards)
        self._K = len(self._reward_df.columns)
        self._T = len(self._reward_df.index)
        self._L = config['L']

        self._policies = []
        self._context = []

        self._seed = config.get('seed')
        if self._seed is not None:
            seed(self._seed)

        self._create_policies(config)

    def _read_context_from_config(self, config):
        """Checks if the current configuration of a policy contains a the path
        to a context csv file. If so it gets appended to self._context,
        otherwise an error gets thrown and the program gets terminated.
        """
        if 'context_path' not in config:
            sys.exit('Missing context_path in config')
        self._context.append(pd.read_csv(config['context_path'], header=None))

    def _create_policies(self, config):
        """Creates the policies based on the configuration and adds them to
        _policies.

        Args:
          config (string): Path to a yaml config file
        """
        for config_for_policy in config['policies']:
            name = config_for_policy['name']

            if name == 'random':
                self._policies.append(RandomPolicy(
                    self._L, self._K, randint(0, 10000)))
                self._context.append(None)
            elif name == 'mpts':
                self._policies.append(
                    MPTS(self._L, self._K, randint(0, 10000)))
                self._context.append(None)
            elif name == 'greedy':
                self._policies.append(
                    EGreedy(self._L, self._K, randint(0, 10000), 0))
                self._context.append(None)
            elif name == 'egreedy':
                self._policies.append(EGreedy(self._L, self._K, randint(
                    0, 10000), _read_epsilon_from_config(config_for_policy)))
                self._context.append(None)
            elif name == 'push-mpts':
                push = config_for_policy['push']
                self._policies.append(PushMPTS(self._L, self._K, randint(
                    0, 10000), self._reward_df.columns.values, push))
                self._context.append(None)
            elif name == 'dkgreedy':
                self._policies.append(DKEGreedy(self._L, self._K, randint(
                    0, 10000), _read_epsilon_from_config(config_for_policy), self._reward_df.columns.values))
                self._context.append(None)
            elif name == 'cdkegreedy':
                self._read_context_from_config(config_for_policy)
                self._policies.append(CDKEGreedy(self._L, self._K, randint(0, 10000), _read_epsilon_from_config(
                    config_for_policy), config_for_policy['alpha'], self._reward_df.columns.values, config_for_policy['q']))
            elif name == 'cpush-mpts':
                self._read_context_from_config(config_for_policy)
                self._policies.append(CPushMpts(self._L, self._K, randint(
                    0, 10000), config_for_policy['push'], config_for_policy['cpush'], self._reward_df.columns.values, config_for_policy['q']))

    def run(self):
        """Performs the experiment. In each of T iterations the policies pick
        L arms and afterwards receive the reward for their choices and the
        maximaliy obtainable reward.
        """
        for i, pol in enumerate(self._policies):
            regret_over_time = [0] * self._T
            total_regret = 0

            for t in range(self._T):
                if isinstance(pol, AbstractContextualBandit):
                    pol.pick_arms(self._context[i].loc[t])
                else:
                    pol.pick_arms()

                picked_arms = pol.get_picked_arms()
                max_reward = sum(sorted(self._reward_df.loc[t])[-self._L:])

                reward_for_arms = self._reward_df.loc[t][self._reward_df.columns.values[picked_arms]]
                pol.learn(reward_for_arms, max_reward)

    def get_policies(self):
        """Getter for _policies

        Returns:
          AbstractBandit[]: Policies of the experiment
        """
        return self._policies

    def get_top_correlated_arms(self, t, names=False):
        """Returns either the names or index of the top L correlated arms for
        iteration t.

        Args:
          t (int): Iteration
          names (bool): If true, return names of the arms, otherwise indicies.
        """
        if names:
            return self._reward_df.columns.values[np.argsort(self._reward_df.loc[t])[-self._L:]]
        else:
            return np.argsort(self._reward_df.loc[t])[-self._L:]

    def get_L(self):
        """Getter for _L

        Returns:
          int: Number of arms to pick each iteration
        """
        return self._L

    def get_K(self):
        """Getter for _K

        Returns:
          int: Number of total arms
        """
        return self._K


def plot_regret_of_policies(experiment):
    """Plots the regret of the different policies evaluated in the experiment
    in one figure.

    Args:
      experiment (Experiment): Run experiment containing the results that will
      be plotted.
    """
    policies = experiment.get_policies()
    fig, axs = plt.subplots(len(policies) + 1, 1,
                            figsize=(15, len(policies) * 7))
    T = 0
    for i, p in enumerate(policies):
        regret = p.get_regret()
        T = len(regret)
        axs[i].plot(list(range(T)), regret)
        axs[i].title.set_text('Regret per round for ' + p.get_name())
        axs[i].set_ylabel('Regret')
        axs[i].set_xlabel('Iteration')
        acc_regret = [0] * T
        for j in range(T):
            acc_regret[j] = acc_regret[j-1] + regret[j]
        axs[len(policies)].plot(list(range(T)), acc_regret, label=p.get_name())
    axs[len(policies)].legend()
    axs[len(policies)].set_ylabel('Regret')
    axs[len(policies)].set_xlabel('Iteration')
    plt.show()


def print_information_about_experiment(experiment):
    """Prints textual information about the experiment.

    Args:
      experiment (Experiment): Run experiment containing the results
    """
    policies = experiment.get_policies()
    total_regret = list(map(lambda policy: sum(policy.get_regret()), policies))
    policy_ranking = np.argsort(
        list(map(lambda regret: -1 * regret, total_regret)))

    no_policies = len(policies)
    print('Experiment: Pick %d arms out of %d' %
          (experiment.get_L(), experiment.get_K()))
    print('A total of %d different policies were evaluated' % no_policies)
    for i, policy_index in enumerate(policy_ranking):
        policy_name = policies[policy_index].get_name()
        policy_regret = total_regret[policy_index]
        print('%d. %s, total regret: %f' %
              (no_policies - i, policy_name, policy_regret))


if __name__ == '__main__':
    # random_regret = Experiment('./experiment_configs/random.yml').run()
    # mpts_regret = Experiment('./experiment_configs/mpts.yml').run()
    scb_regret = Experiment('./experiment_configs/scb-mpts.yml').run()

    # fig = plt.figure()
    # ax = plt.axes()

    # ax.plot(range(len(random_regret)),random_regret,label='random')
    # ax.plot(range(len(mpts_regret)), mpts_regret, label='mpts')
    # ax.plot(range(len(scb_regret)), scb_regret, label='scb')


def perform_experiment_for_L(config, L, plot=True):
    """Performs an experiment given a config and L and prints information about
    the experiment. If desired, outcomes of the experiment get plotted as well.

    Args:
      config (dict): Configuration of the experiment
      L (int): Number of arms to pick each iteration.
      plot (bool): If True outcome of experiment gets plotted.
    """
    e = Experiment(additional_config=config | {'L': L})
    e.run()
    print_information_about_experiment(e)
    if plot:
        plot_regret_of_policies(e)


def perform_experiment_for_Ls(config, Ls, plot=True):
    """Performs the same experiment for different values of L.

    Args:
      config (dict): Configuration for the experiment
      Ls (int[]): Different values of L for which the experiment is run.
      plot (bool): If true, outcome of experiment gets plotted.
    """
    for L in Ls:
        perform_experiment_for_L(config, L, plot)


def perform_top_L_experiment_for_L(config, filepath, L, plot=True):
    """Like perform_experiment_for_L, but adjusts the filepath to load the
    proper reward file for the experiment based on L.

    Args:
      config (dict): Configuration for the experiment
      filepath (string): Filepath with a placeholder %s for L.
      L (int): Arms to pick in the experiment
      plot (bool): If true, outcome of experiment gets plotted.
    """
    filepath = filepath % str(L)
    perform_experiment_for_L(config | {'reward_path': filepath}, L, plot)


def perform_top_L_experiment_for_Ls(config, filepath, Ls, plot=True):
    """Like perform_experiment_for_Ls, but adjusts the filepath to load their
    proper reward file for the experiment based on L.

    Args:
      config (dict): Configuration for the experiment
      filepath (string): Filepath with a placeholder %s for L.
      Ls (int[]): Arms to pick in the experiment
      plot (bool): If true, outcome of experiment gets plotted.
    """
    for L in Ls:
        perform_top_L_experiment_for_L(config, filepath, L, plot)
